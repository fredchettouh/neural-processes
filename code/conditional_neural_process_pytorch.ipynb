{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederik/Google Drive/University/UC3M/TFM/cnp_repo/neural-processes/tfm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/frederik/Google Drive/University/UC3M/TFM/cnp_repo/neural-processes/tfm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/frederik/Google Drive/University/UC3M/TFM/cnp_repo/neural-processes/tfm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/frederik/Google Drive/University/UC3M/TFM/cnp_repo/neural-processes/tfm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/frederik/Google Drive/University/UC3M/TFM/cnp_repo/neural-processes/tfm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/frederik/Google Drive/University/UC3M/TFM/cnp_repo/neural-processes/tfm/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_shift_uniform(a=0,b=1,*size):\n",
    "    return torch.rand(size=(size))*(a-b)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNPRegressionDescription = collections.namedtuple(\n",
    "    'CNPRegressionDescription',\n",
    "    ('query',\n",
    "     'target_y',\n",
    "     'num_total_points',\n",
    "     'num_context_points'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow of the implementation\n",
    "\n",
    "- CNPS as Gaussian Processes try to learn a distribution over the functional value vector $V=(f(x_1).....f(x_n))$\n",
    "- At test time any function from this distribution can be approximated\n",
    "- The function will take into consideration the context points that have beeen give to make one function from this distribution more likely than others.\n",
    "\n",
    "### Data Generation:\n",
    "- The training points come from various functions that share some common characteristic\n",
    "- In this implemenation the different functions come from __one__ Gaussian process\n",
    "- A GP is a multivariate normal distribution, aka a mean and covariance matrix, where each dimension of the infinite random vector is a, aka random variable, is the functional value for a given input value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Kernel - __Creates a covariance matrix__: A function that takes in x values and returns a co-variance matrix. Here the Gaussian Kernel, RBF, or squared exponential is used. It computes the differences between all one dimensional feature vectors scales this distance by ``l``, squares it and scales it again by ``sigma_f``. As seen in other implementations (<a href = https://www.inf.ed.ac.uk/teaching/courses/mlpr/2019/notes/gp_minimal.py> GP demo</a>) some noises is added to the diagonal or to the variance of the covariance matrix to ensure a positive definite matrix and thus allow the Cholesky decomposition to be defined.\n",
    "<br><br>\n",
    "2. Curve Generator - __Generates functions from a GP__: We first set ``num_context_points`` <br>\n",
    "__Training__: The number of target points is a random share of the context points. Random ``x_values`` are generated from a ``uniform`` between `-2` and `2`. Each of the ``batch_size`` vectors of context points is 1 by ``num_context_points``.<br>\n",
    "__Testing__:For testing more ``targets`` (400) are created and are simply set at ``0.001`` intervals between `-2` and `2`.  <br><br>\n",
    "The Kernel scale parameters are set and the ``x_values`` are past through the ``kernel`` to create the covariance matrix. The covariance matrix is decomposed with the ``cholesky``decomposition. The ``y_values`` are created through the following process:\n",
    "Given the standardization of a non-standard multivariate normal $Z=\\frac{X-\\mu}{\\sigma}$ we can create the non-standard multivariate normal by $L^-1*Z+\\mu=Z$ where $L^-1$ is the Cholesky.\n",
    "\n",
    "Finally, depending on training/ testing the appropriate number of points are selected from the ``x_values`` and the ``y_values``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, predicting with GPs:\n",
    "\n",
    "GPs during training build the posterior distribution conditioned on the observed data.\n",
    "At test time this distribution serves as the prior which will then be updated using the Bayes rule. Drawing from this distribution or taking the expected value is the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Function for Gaussian Process:\n",
    "- The Covariance function encodes our believes about the function to be generated/ learned \n",
    "- It encodes notion of similarity, i.e. similar inputs produce similar outputs\n",
    "- Different factors influence the choice of the covariance function\n",
    "- __Stationary covariance__ function is invariant to translations on the input space\n",
    " - I guess this it is constant when the inputs are transformed \n",
    "- __Isotropy or isotropic__ covariance functions are functions only of $|X_1-X_2|$, i.e. $K(X_1,X_2)$.\n",
    "- The function only depends on the distance between the two input vectors\n",
    "- This implementation uses the squared expnential, aka Radial Basis Function aka Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Curve Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPCurvesREader(object):\n",
    "    def __init__(self,\n",
    "                batch_size,\n",
    "                max_num_context,\n",
    "                x_size=1,\n",
    "                y_size=1,\n",
    "                l1_scale=0.4,\n",
    "                sigma_scale=1.0,\n",
    "                testing=False):\n",
    "\n",
    "        self._batch_size = batch_size \n",
    "        #batch_size seems to be the number of batches\n",
    "        #input is a (batch_size,rows,col, i.e. 64 batches with 10, 1D datapoints)\n",
    "        self._max_num_context = max_num_context\n",
    "        self._x_size = x_size \n",
    "        self._y_size = y_size\n",
    "        self._l1_scale = l1_scale\n",
    "        self._sigma_scale = sigma_scale\n",
    "        self._testing = testing\n",
    "    \n",
    "    def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "        \"\"\"Generates curves from a Gaussian Process\n",
    "            GP is a distribution over a vector of functional values\n",
    "        \n",
    "        Args:\n",
    "            xdata: Tensor with shape `[batch_size, num_total_points, x_size]` with\n",
    "          the values of the x-axis data.\n",
    "      l1: Tensor with shape `[batch_size, y_size, x_size]`, the scale\n",
    "          parameter of the Gaussian kernel.\n",
    "      sigma_f: Float tensor with shape `[batch_size, y_size]`; the magnitude\n",
    "          of the std.\n",
    "      sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "    Returns:\n",
    "      The kernel, a float tensor with shape\n",
    "      `[batch_size, y_size, num_total_points, num_total_points]`.\n",
    "    \"\"\"\n",
    "        \n",
    "#       number of points per batch\n",
    "        num_total_points = xdata.shape[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        xdata1 = torch.unsqueeze(xdata, axis=1)\n",
    "        xdata2 = torch.unsqueeze(xdata, axis=2)\n",
    "#       takes the pairwise difference between for each of the data points in each batch\n",
    "        diff = xdata1- xdata2\n",
    "#       the squared exponential is defined as the sum of the squared differences scaled by a factor gamma\n",
    "#       l1 is simply the length parameter of the Kernel\n",
    "        norm = (diff[:, None, :, :, :] / l1[:, :, None, None, :]).pow(2)\n",
    "        \n",
    "        norm = norm.squeeze(-1)\n",
    "#       we now have a GP or a description of a multivariate normal distribution\n",
    "#       from which we can generate functions\n",
    "        kernel = sigma_f.pow(2)[:, :, None, None]*torch.exp(-0.5*norm)\n",
    "        \n",
    "        kernel += (sigma_noise**2)*torch.eye(num_total_points)\n",
    "        \n",
    "        return kernel\n",
    "        \n",
    "    def generate_curves(self):\n",
    "        \n",
    "        \"\"\"context points are the points used for training - chosen at random\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        num_context = np.random.randint(low=3,\n",
    "                                    high=self._max_num_context,\n",
    "                                    dtype=np.int32)\n",
    "        if self._testing:\n",
    "#           we are going to generating 400 points to predict\n",
    "            num_target = 400\n",
    "            num_total_points = num_target\n",
    "#           the x-axis is a tensor of depth batch_size \n",
    "#           num_target lentgh and width of 1  \n",
    "            x_values = torch.arange(-2.,2.,1./100, dtype=torch.float32).repeat(1,64)\n",
    "            x_values = torch.unsqueeze(-1)\n",
    "        \n",
    "        else:\n",
    "#           for training only max_num_context points are generated\n",
    "#           \n",
    "            num_target = np.random.randint(low=2,\n",
    "                                       high=self._max_num_context,\n",
    "                                       dtype=np.int32)\n",
    "    \n",
    "            num_total_points = num_context + num_target\n",
    "            x_values = scale_shift_uniform(-2,2,\n",
    "                                           self._batch_size,\n",
    "                                           num_total_points,\n",
    "                                           self._x_size)\n",
    "#       setting gamma and sigma        \n",
    "        l1 = torch.ones(size=(self._batch_size,\n",
    "                       self._x_size,\n",
    "                       self._y_size))*self._l1_scale\n",
    "        sigma_f = torch.ones(size=(self._batch_size,\n",
    "                                   self._y_size))*self._sigma_scale\n",
    "        \n",
    "        kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "        \n",
    "        cholesky = torch.cholesky(kernel.double()).float()\n",
    "        \n",
    "        standard_normals = torch.randn(\n",
    "            size=(\n",
    "                self._batch_size,\n",
    "                self._y_size,\n",
    "                num_total_points,1))\n",
    "        \n",
    "        y_values = torch.matmul(cholesky,standard_normals)\n",
    "        print(y_values.shape)\n",
    "        y_values = torch.transpose(y_values.squeeze(3),2,1)\n",
    "        \n",
    "        \n",
    "        if self._testing:\n",
    "            # Select the targets\n",
    "            target_x = x_values\n",
    "            target_y = y_values\n",
    "            idx = torch.randperm(num_target)\n",
    "            x_values.shape\n",
    "            context_x = x_values[:,idx[:num_context],:]\n",
    "            context_y = y_values[:,idx[:num_context],:]\n",
    "            \n",
    "        else:\n",
    "#             if not we have values that serve as context and targets\n",
    "            target_x = x_values[:,:num_target + num_context:]\n",
    "            target_y = y_values[:,:num_target + num_context:]\n",
    "#             select target points\n",
    "            context_x = x_values[:, :num_context, :]\n",
    "            context_y = y_values[:, :num_context, :]\n",
    "        \n",
    "        query = ((context_x, context_y), target_x)\n",
    "        \n",
    "        return CNPRegressionDescription(\n",
    "            query=query,\n",
    "            target_y=target_y,\n",
    "            num_total_points=target_x.shape[1],\n",
    "            num_context_points=num_context)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder creates a representation of the data context data. Some technicalilites to understand are the following: \n",
    "- The encoder simply takes a concatenation of the x,y values\n",
    "- x values are of shape batch_size*number of context points, dimx\n",
    "- y values are of shape batch_size*number of context points, 1\n",
    "- the represenation ri can be of the dimensions that we choose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    \n",
    "    \"\"\"This class maps each x_i, y_i context point to a representation r_i\n",
    "    To learn this Representation we are using a Multi Layer Perceptron\n",
    "    The input shape will be batch_size, num_context_points, x_dim\n",
    "    \n",
    "    The input to the encoder are the value pairs, thus the dimensions are \n",
    "    Batch_Size, (dimx+dimy). The Pytorch automatically pases the values sequentially\n",
    "    through the ANN.\n",
    "    The last layer will not have an activation function because we want the pure represenation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dimx : int\n",
    "        Dimesion of each x value\n",
    "    \n",
    "    dimy : int\n",
    "        Dimesion of each y value\n",
    "        \n",
    "    dimr : int\n",
    "        Dimension of output representation\n",
    "    \n",
    "    dimh : tuple\n",
    "        Dimension of hidden layers\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, dimx, dimy, dimr, *args):\n",
    "        super().__init__()\n",
    "            \n",
    "        self._dimx = dimx\n",
    "        self._dimy = dimy\n",
    "        self._dimr = dimr\n",
    "        self._dimh = args\n",
    "        \n",
    "        \n",
    "        _first_layer = [nn.Linear(self._dimx+self._dimy, self._dimh[0]),nn.ReLU()]\n",
    "        \n",
    "        \n",
    "        _hidden_layers = list(np.array([\n",
    "            [nn.Linear(self._dimh[i], self._dimh[i+1]),nn.ReLU()]\n",
    "            for i in range(len(self._dimh)-2)\n",
    "        ]).flatten())\n",
    "        \n",
    "        _last_layer = [nn.Linear(self._dimh[-2], self._dimh[-1])]\n",
    "        \n",
    "        self._layers = _first_layer + _hidden_layers + _last_layer\n",
    "        \n",
    "        \n",
    "        self._process_input = nn.Sequential(*self._layers)\n",
    "        \n",
    "    def forward(self, x_values, y_values):\n",
    "        \"\"\"\n",
    "        Takes the context points x and y,\n",
    "        concatenates them into value pairs\n",
    "        and passes them through the MLP\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        x_values : torch.Tensor \n",
    "            Shape (batch_size, dimx)\n",
    "            \n",
    "        y_values : torch.Tensor \n",
    "            Shape (batch_size, dimy)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        input_as_pairs = torch.cat((x_values, y_values), dim=1)\n",
    "\n",
    "        \n",
    "        return self._process_input(input_as_pairs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator\n",
    "\n",
    "- The aggregator simply creates an aggregation'\n",
    "- Here we simply take the average of ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(ri_tensor):\n",
    "    \"\"\"Takes a tensor of shape (batch_size,num_context_points, dimr) and aggregates it a\n",
    "    along the second axis so that we have an aggregation across each batch\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    ri_tensor : Tensor\n",
    "        Tensor of the representation of the x and y context points\n",
    "    \"\"\"\n",
    "    \n",
    "    return ri_tensor.mean(dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "- The decoder takes the context points and the representation, passes them through an MLP and returns an output of dimensions two\n",
    "- These two are used to minimize the negative log conditinal ligelihood which is a function that depends on the mu and sigma\n",
    "- By minimize this quantitiy we find the correct parameters to the distribution of the Gaussian process from which the context points were drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \"\"\"The decoder takes in x_values, that is the target points and combines them with\n",
    "    the represenation of the context points by concatenation. The resulting tensor is passed to an MLP that \n",
    "    is asked to ouput the parameters for the sought after distribution, in this case\n",
    "    a normal distribution. Thus we are looking for two parameters. The MLP returns two tensor obejects\n",
    "    which hold a mean/ variance for each point y. Thus the shape of this output is \n",
    "    batch_size,y_values,y_dim, 2\n",
    "    \n",
    "    Note the targets consist\n",
    "    of both the context points as well as the target points, since the context points\n",
    "    are a subset of the target points.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        \n",
    "    dimx : int\n",
    "        Dimension of each x value\n",
    "    \n",
    "    dimr : int\n",
    "        Dimension of each of the representations\n",
    "    \n",
    "    *args : tuple\n",
    "        Dimensions of the hidden layers \n",
    "    \n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dimx, dimr,dimparam,*args):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self._dimx = dimx\n",
    "        self._dimr = dimr\n",
    "        self._dimparam = dimparam\n",
    "        self._dimh = args\n",
    "        \n",
    "        _first_layer = [nn.Linear(self._dimx+self._dimr, self._dimh[0]),nn.ReLU()]\n",
    "        \n",
    "        \n",
    "        _hidden_layers = list(np.array([\n",
    "            [nn.Linear(self._dimh[i], self._dimh[i+1]),nn.ReLU()]\n",
    "            for i in range(len(self._dimh)-1)\n",
    "        ]).flatten())\n",
    "        \n",
    "        _last_layer = [nn.Linear(self._dimh[-1], self._dimparam)]\n",
    "        \n",
    "        self._layers = _first_layer + _hidden_layers + _last_layer\n",
    "        \n",
    "        \n",
    "        self._process_input = nn.Sequential(*self._layers)\n",
    "        \n",
    "    def forward(self, x_values,r_values):\n",
    "        \n",
    "        \"\"\"Takes x and r values, combines them and passes them twice to MLP. \n",
    "        Thus we have one run for mu and one run for sigma\"\"\"\n",
    "        \n",
    "        \n",
    "        input_as_pairs = torch.cat((x_values, r_values),dim=1)\n",
    "        \n",
    "        dist_params = self._process_input(input_as_pairs)\n",
    "        return self._process_input(input_as_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Sandbox - looking at some tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel function\n",
    "Expanding the dimesions of the tensors allows us substract each point pairwise\n",
    "This being a 1-D use case we simply substract each of the data points and get an n by n matrix for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = int(2e5)\n",
    "MAX_CONTEXT_POINTS = 10\n",
    "PLOT_AFTER = int(2e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset_train = GPCurvesREader(\n",
    "    batch_size=64, \n",
    "    max_num_context=MAX_CONTEXT_POINTS)\n",
    "data_train = dataset_train.generate_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_points = 10\n",
    "dimx = 1\n",
    "dimy = 1\n",
    "rdim = 20\n",
    "hdim = 20\n",
    "x = scale_shift_uniform(-2,2,batch_size,num_points,dimx)\n",
    "y = scale_shift_uniform(-2,2,batch_size,num_points,dimy)\n",
    "\n",
    "x_stacked  = x.view(batch_size*num_points,-1)\n",
    "y_stacked  = y.view(batch_size*num_points,-1)\n",
    "\n",
    "encoder = Encoder(1,1,hdim,hdim,hdim,rdim)\n",
    "r  = encoder.forward(x_stacked,y_stacked).view(batch_size,num_points,-1)\n",
    "r_aggregate = aggregate(r).unsqueeze(1)\n",
    "r_aggregate = r_aggregate.repeat(1,num_points,1)\n",
    "\n",
    "hdim = 128\n",
    "outdim = 2\n",
    "decoder = Decoder(dimx,rdim,outdim,hdim,hdim,hdim)\n",
    "\n",
    "r_stacked = r_aggregate.view(batch_size*num_points,-1)\n",
    "\n",
    "dist_params = decoder.forward(x_stacked, r_stacked).view(batch_size,num_points,-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "tfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
